{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790c9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "abbr_map = {\n",
    "    \"ACT\":   \"Acceptance and commitment therapy\",\n",
    "    \"ADHD\":  \"Attention-deficit hyperactivity disorder\",\n",
    "    \"AI\":    \"Artificial intelligence\",\n",
    "    \"BA\":    \"Behavioural activation\",\n",
    "    \"CAM\":   \"Complementary and alternative medicine\",\n",
    "    \"CANMAT\":\"Canadian Network for Mood and Anxiety Treatments\",\n",
    "    \"CBASP\": \"Cognitive behavioural analysis system of psychotherapy\",\n",
    "    \"CBT\":   \"Cognitive-behavioural therapy\",\n",
    "    \"CPD\":   \"Continuing professional development\",\n",
    "    \"CYP\":   \"Cytochrome P450\",\n",
    "    \"DBS\":   \"Deep brain stimulation\",\n",
    "    \"DHI\":   \"Digital health intervention\",\n",
    "    \"DLPFC\": \"Dorsolateral prefrontal cortex\",\n",
    "    \"DSM-5-TR\": \"Diagnostic and Statistical Manual, 5th edition, Text Revision\",\n",
    "    \"DSM-IV-TR\":\"Diagnostic and Statistical Manual, 4th edition, Text Revision\",\n",
    "    \"DTD\":   \"Difficult-to-treat depression\",\n",
    "    \"ECG\":   \"Electrocardiography\",\n",
    "    \"ECT\":   \"Electroconvulsive therapy\",\n",
    "    \"EEG\":   \"Electroencephalography\",\n",
    "    \"GRADE\": \"Grading of Recommendations Assessment, Development, and Evaluation\",\n",
    "    \"ICD\":   \"International Classification of Diseases\",\n",
    "    \"IPT\":   \"Interpersonal therapy\",\n",
    "    \"MAOI\":  \"Monoamine oxidase inhibitor\",\n",
    "    \"MBC\":   \"Measurement-based care\",\n",
    "    \"MBCT\":  \"Mindfulness-based cognitive therapy\",\n",
    "    \"MCT\":   \"Metacognitive therapy\",\n",
    "    \"MDD\":   \"Major depressive disorder\",\n",
    "    \"MDE\":   \"Major depressive episode\",\n",
    "    \"MI\":    \"Motivational interviewing\",\n",
    "    \"MST\":   \"Magnetic seizure therapy\",\n",
    "    \"NbN\":   \"Neuroscience-based nomenclature\",\n",
    "    \"NDRI\":  \"Norepinephrine-dopamine reuptake inhibitor\",\n",
    "    \"NMDA\":  \"N-methyl-D-aspartate\",\n",
    "    \"NSAID\": \"Nonsteroidal anti-inflammatory drug\",\n",
    "    \"PDD\":   \"Persistent depressive disorder\",\n",
    "    \"PDT\":   \"Psychodynamic psychotherapy\",\n",
    "    \"PHQ\":   \"Patient health questionnaire\",\n",
    "    \"PST\":   \"Problem-solving therapy\",\n",
    "    \"RCT\":   \"Randomized controlled trial\",\n",
    "    \"rTMS\":  \"Repetitive transcranial magnetic stimulation\",\n",
    "    \"SDM\":   \"Shared decision-making\",\n",
    "    \"SNRI\":  \"Serotonin-norepinephrine reuptake inhibitor\",\n",
    "    \"SSRI\":  \"Selective serotonin reuptake inhibitor\",\n",
    "    \"STPP\":  \"Short-term psychodynamic psychotherapy\",\n",
    "    \"TBS\":   \"Theta burst stimulation\",\n",
    "    \"TCA\":   \"Tricyclic antidepressants\",\n",
    "    \"tDCS\":  \"Transcranial direct current stimulation\",\n",
    "    \"TMS\":   \"Transcranial magnetic stimulation\",\n",
    "    \"TRD\":   \"Treatment-resistant depression\",\n",
    "    \"USA\":   \"United States of America\",\n",
    "    \"VNS\":   \"Vagus nerve stimulation\",\n",
    "    \"WHO\":   \"World Health Organization\",\n",
    "}\n",
    "\n",
    "# append definition to abbreviation\n",
    "def append_definition(match: re.Match) -> str:\n",
    "    abbr = match.group(1)\n",
    "    definition = abbr_map.get(abbr, \"\")\n",
    "    return f\"{abbr} ({definition})\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e9738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "llm_client = Together(api_key='4f6e44b7689d6592b2b5b57ad3940ac9f488d14c22802e8bcdf641b06e98cbbe')\n",
    "#4f6e44b7689d6592b2b5b57ad3940ac9f488d14c22802e8bcdf641b06e98cbbe\n",
    "\n",
    "def img_to_text(image_path: str) -> str:\n",
    "\n",
    "    response = llm_client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-Vision-Free\", #don't change the model!\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\",\n",
    "                \"text\": \"please extract the text in this img. If there is a table, extract it to csv format.\"},\n",
    "                {\"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": \"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/843a/11351064/e8dc176dd369/10.1177_07067437241245384-table1.jpg\"}}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=500\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "print(img_to_text(\"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/843a/11351064/e8dc176dd369/10.1177_07067437241245384-table1.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c188fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "chunk_id = 1\n",
    "\n",
    "filename = \"../data/raw/source.html\"\n",
    "# if there's no input from command line, use the default filename\n",
    "# if len(sys.argv) > 1:\n",
    "    # filename = sys.argv[1]\n",
    "    \n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "\n",
    "# # I manually substitute the Level 1, Level 2, Level 3, Level 4 link with text like (Leve 1), (Level 2), (Level 3), (Level 4)\n",
    "level1 = 'https://cdn.ncbi.nlm.nih.gov/pmc/blobs/843a/11351064/62befe587468/10.1177_07067437241245384-img1.jpg'\n",
    "level2 = 'https://cdn.ncbi.nlm.nih.gov/pmc/blobs/843a/11351064/b9ea5ad77490/10.1177_07067437241245384-img2.jpg'\n",
    "level3 = 'https://cdn.ncbi.nlm.nih.gov/pmc/blobs/843a/11351064/5be38aafe33f/10.1177_07067437241245384-img3.jpg'\n",
    "level4 = 'https://cdn.ncbi.nlm.nih.gov/pmc/blobs/843a/11351064/68e56cd87632/10.1177_07067437241245384-img4.jpg'\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "output = []\n",
    "\n",
    "\n",
    "# parse the h1 title\n",
    "title = soup.find(\"h1\")\n",
    "if title:\n",
    "    title = title.decode_contents().replace('\\n', '')\n",
    "    \n",
    "output.append({\n",
    "    \"text\": title,\n",
    "    \"metadata\":{\n",
    "    \"section\": \"title\",\n",
    "    \"type\": \"title\",\n",
    "    \"chunk_index\": chunk_id,\n",
    "    \"headings\": \"Title\",\n",
    "    \"referenced_tables\": [],\n",
    "    }\n",
    "})\n",
    "\n",
    "chunk_id += 1\n",
    "\n",
    "countlevel1 = 0\n",
    "countlevel2 = 0\n",
    "countlevel3 = 0\n",
    "countlevel4 = 0\n",
    "# parse the main body\n",
    "for p in soup.find_all(\"p\"):\n",
    "    referenced_tables = set()\n",
    "    \n",
    "    #-----------------------replace the <img> tags---------------------\n",
    "    # we also manually delete the duplication in first occurrence mentioning Levels\n",
    "    for img in p.find_all('img'):\n",
    "        src = img.get('src')\n",
    "        if src == level1:\n",
    "            replacement_text = \"(Level 1)\"\n",
    "            countlevel1 += 1\n",
    "        elif src == level2:\n",
    "            replacement_text = \"(Level 2)\"\n",
    "            countlevel2 += 1\n",
    "        elif src == level3:\n",
    "            replacement_text = \"(Level 3)\"\n",
    "            countlevel3 += 1\n",
    "        elif src == level4:\n",
    "            replacement_text = \"(Level 4)\"\n",
    "            countlevel4 += 1\n",
    "        else:\n",
    "            continue    \n",
    "            \n",
    "        text_node = NavigableString(replacement_text)\n",
    "        img.replace_with(text_node)\n",
    "        referenced_tables.add('Table A')\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    # ----------------------get section id----------------------------------\n",
    "    parent_sec = p.find_parent([\"section\",'figure'], id=True)\n",
    "    sec_id = parent_sec.get(\"id\") if parent_sec else None\n",
    "    \n",
    "    #TODO: I'm currently ignoring the HTML tables, becuase AIKA's processing them.\n",
    "    \n",
    "    \n",
    "    #-----------------------get the headings---------------------\n",
    "    # special case: # manually finding the \"No heading\" in the html file to fix the No heading issue\n",
    "    # delete the <div><\\div> outside this to get the correct heading: <p>Protracted Discontinuation Symptoms and Hyperbolic Tapering Schedules.</p>\n",
    "    \n",
    "    # get the closest heading\n",
    "    heading = p.find_previous_sibling(lambda tag: bool(re.match(r'^h[2-6]$', tag.name)))\n",
    "    headings = heading.get_text(strip=True) if heading else 'No heading' \n",
    "    if 'fig' in sec_id:\n",
    "        referenced_tables.add(headings)\n",
    "    #while parent still has parents\n",
    "    while parent_sec:\n",
    "        # print(f\"parent_sec: {parent_sec.get('id')}\")\n",
    "        heading = parent_sec.find_previous_sibling(lambda tag: bool(re.match(r'^h[2-6]$', tag.name)))\n",
    "        if heading:\n",
    "            headings = heading.get_text(strip=True) + ' > ' + headings\n",
    "        parent_sec = parent_sec.find_parent(\"section\", id=True)\n",
    "\n",
    "    headings = headings.strip().replace('\\n', ' ')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    #-----------------------get the text---------------------\n",
    "    text = p.get_text(separator=' ', strip=True) # get only text\n",
    "    # text = p.decode_contents().replace('\\n', ' ') #get the text with <href> links and other tags\n",
    "    \n",
    "    \n",
    "    #----------------------- mark Type of the text ------------------------\n",
    "    type = 'paragraph'\n",
    "    \n",
    "    if 'table' in sec_id:  # either table image or table in HTML format\n",
    "        if p.get('class') and 'img-box' in p.get('class'):\n",
    "            type = 'table image '\n",
    "        else: \n",
    "            continue\n",
    "        img_link = p.find('img').get('src')\n",
    "        # print(\"img_link: \", img_link)\n",
    "        ############################################\n",
    "        # --------------------- use vision model to get the text from the image ------------------------\n",
    "        response = llm_client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        # Change prompt here?\n",
    "                        {\"type\": \"text\", \"text\": \"Extract the text in this img. If there is a table, extract it to csv format. Only return the text in csv format, no other explanation.\"},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": link}}\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=3000,\n",
    "            temperature=0.05\n",
    "        )\n",
    "        parsed_results = response.choices[0].message.content\n",
    "        print(\"parsed_results: \", parsed_results)\n",
    "        # --------------------- use vision model to get the text from the image ------------------------\n",
    "        \n",
    "        # put the response directly to json\n",
    "        text = str(parsed_results)\n",
    "        ############################################\n",
    "        # text = str(img_link)\n",
    "        \n",
    "\n",
    "        # I only have 2 here with parent = <figure>, most of img's parent are <section id = 'table...'>\n",
    "    elif p.get('class') and 'img-box' in p.get('class'):\n",
    "        type = 'figure image'\n",
    "        img_link = p.find('img').get('src')\n",
    "        \n",
    "        # print(\"img_link: \", img_link)\n",
    "        ############################################\n",
    "        ############################################\n",
    "\n",
    "        text = str(img_link)\n",
    "        \n",
    "        # sec_id = p.find_parent(\"figure\", id=True).get(\"id\")\n",
    "    elif 'box' in sec_id: \n",
    "        type = 'box'\n",
    "    \n",
    "    \n",
    "    \n",
    "    #----------------------- get referenced tables ------------------------\n",
    "    all_links = p.find_all('a')\n",
    "    for link in all_links:\n",
    "        href = link.get('href')\n",
    "        if href.startswith('#'):\n",
    "            referenced_tables.add(link.get_text(strip=True))\n",
    "            \n",
    "    # ----------------------- replace abbreviation with definition ------------------------\n",
    "    # \\b(ACT|ADHD|AI|…)\\b\n",
    "    pattern = re.compile(\n",
    "        r'\\b(' + '|'.join(re.escape(k) for k in abbr_map.keys()) + r')\\b'\n",
    "    )\n",
    "    text = pattern.sub(append_definition, text)\n",
    "    \n",
    "            \n",
    "    #----------------------- formate the chunks ------------------------\n",
    "    chunk = {\n",
    "        \"text\": \"From section: \"+ headings + \" > paragraph id: \" + str(chunk_id) + \"\\n\"+ text,\n",
    "        \"metadata\": {\n",
    "        \"section\": \"https://pmc.ncbi.nlm.nih.gov/articles/PMC11351064/#\" + sec_id,\n",
    "        \"type\": type,\n",
    "        \"chunk_index\": chunk_id,\n",
    "        \"headings\": headings,\n",
    "        \"referenced_tables\": list(referenced_tables),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output.append(chunk)\n",
    "    \n",
    "    chunk_id += 1\n",
    "    \n",
    "    \n",
    "print(\"count of levels: \", countlevel1, countlevel2, countlevel3, countlevel4)\n",
    "    \n",
    "# ----------------------- write to json ------------------------\n",
    "with open(\"../data/guideline_db.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"output.json file created with {len(output)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c32ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e94211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Text Extraction:**\n",
      "\n",
      "The image contains a table with various medications and their corresponding information. The text in the image is as follows:\n",
      "\n",
      "*   Line of treatment\n",
      "*   Antidepressant\n",
      "*   Daily dose\n",
      "*   Mechanism\n",
      "*   Level of evidence\n",
      "\n",
      "**Table Extraction in CSV Format:**\n",
      "\n",
      "Here is the table extracted in CSV format:\n",
      "\n",
      "\"Line of treatment\",\"Antidepressant\",\"Daily dose\",\"Mechanism\",\"Level of evidence\"\n",
      "\"First line\",\"Citalopram\",\"20–40 mg\",\"SSRI\",\"\"\n",
      "\"First line\",\"Escitalopram\",\"10–20 mg\",\"SSRI\",\"\"\n",
      "\"First line\",\"Fluoxetine\",\"20–60 mg\",\"SSRI\",\"\"\n",
      "\"First line\",\"Fluvoxamine\",\"100–300 mg\",\"SSRI\",\"\"\n",
      "\"First line\",\"Paroxetine\",\"20–50 mg\",\"SSRI\",\"\"\n",
      "\"First line\",\"Sertraline\",\"50–200 mg\",\"SSRI\",\"\"\n",
      "\"First line\",\"Desvenlafaxine\",\"50–100 mg\",\"SNRI\",\"\"\n",
      "\"First line\",\"Duloxetine\",\"60–120 mg\",\"SNRI\",\"\"\n",
      "\"First line\",\"Levomilnacipran*\",\"40–120 mg\",\"SNRI\",\"\"\n",
      "\"First line\",\"Venlafaxine-XR\",\"75–225 mg\",\"SNRI\",\"\"\n",
      "\"First line\",\"Bupropion\",\"150–450 mg\",\"NDRI\",\"\"\n",
      "\"First line\",\"Mirtazapine\",\"30–60 mg\",\"α2 antagonist; 5-HT2 antagonist\",\"\"\n",
      "\"First line\",\"Vilazodone*\",\"20–40 mg\",\"SRI; 5-HT1A agonist\",\"\"\n",
      "\"First line\",\"Vortioxetine\",\"10–20 mg\",\"SRI; 5-HT1A, 5-HT1B agonist; 5-HT1D, 5-HT3A, 5-HT7 antagonist\",\"\"\n",
      "\"First line\",\"Agomelatine #\",\"25–50 mg\",\"MT1, MT2 agonist; 5-HT2 antagonist\",\"\"\n",
      "\"First line\",\"Mianserin #\",\"30–90 mg\",\"α2 antagonist; 5-HT2 antagonist\",\"\"\n",
      "\"First line\",\"Milnacipran #\",\"50–200 mg\",\"SNRI\",\"\"\n",
      "\"Second line\",\"Amitriptyline\",\"75–300 mg\",\"TCA\",\"\"\n",
      "\"Second line\",\"Clomipramine\",\"150–300 mg\",\"TCA\",\"\"\n",
      "\"Second line\",\"Desipramine\",\"100–300 mg\",\"TCA\",\"\"\n",
      "\"Second line\",\"Doxepin\",\"75–300 mg\",\"TCA\",\"\"\n",
      "\"Second line\",\"Imipramine\",\"75–300 mg\",\"TCA\",\"\"\n",
      "\"Second line\",\"Nortriptyline\",\"75–150 mg\",\"TCA\",\"\"\n",
      "\"Second line\",\"Protriptyline\",\"30–60 mg\",\"TCA\",\"\"\n",
      "\"Second line\",\"Trimipramine\",\"75–300 mg\",\"TCA\",\"\"\n",
      "\"Second line\",\"Moclobemide\",\"150–450 mg\",\"RIMA\",\"\"\n",
      "\"Second line\",\"Trazodone\",\"150–400 mg\",\"SRI; 5-HT2 antagonist\",\"\"\n",
      "\"Second line\",\"Quetiapine\",\"150–300 mg\",\"DA, 5-HT, α1 & α2 antagonist; NRI\",\"\"\n",
      "\"Second line\",\"Dextromethorphan-bupropion* #\",\"45mg/105mg-90mg/210mg\",\"NMDA antagonist; NDRI, sigma-1 agonist\",\"\"\n",
      "\"Third line\",\"Nefazodone #\",\"300–600 mg\",\"SRI, 5-HT2 antagonist\",\"\"\n",
      "\"Third line\",\"Selegiline transdermal #\",\"6–12 mg\",\"MAO-B inhibitor\",\"\"\n",
      "\"Third line\",\"Phenelzine\",\"45–90 mg\",\"MAO inhibitor\",\"\"\n",
      "\"Third line\",\"Tranylcypromine\",\"30–60 mg\",\"MAO inhibitor\",\"\"\n",
      "\"Third line\",\"Reboxetine #\",\"8–12 mg\",\"NRI\",\"\"\n",
      "\n",
      "Note: The table has been extracted in CSV format, with each row representing a single entry in the table. The columns are separated by commas, and the rows are separated by newline characters.\n"
     ]
    }
   ],
   "source": [
    "from together import Together\n",
    "llm_client = Together(api_key='4f6e44b7689d6592b2b5b57ad3940ac9f488d14c22802e8bcdf641b06e98cbbe')\n",
    "#4f6e44b7689d6592b2b5b57ad3940ac9f488d14c22802e8bcdf641b06e98cbbe\n",
    "\n",
    "model = \"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\"\n",
    "link = \"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/843a/11351064/eb6b31476a11/10.1177_07067437241245384-table12.jpg\"\n",
    "\n",
    "# set the temperature of the model to 0.05\n",
    "response = llm_client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Extract the text in this img. If there is a table, extract it to csv format. Only return the text in csv format, no other explanation.\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": link}}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=3000,\n",
    "    temperature=0.05\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e87341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('../data/to_excel.json')\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel('output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ffdac",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# all_tables.append(tables)\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# print(tables)\u001b[39;00m\n\u001b[32m     21\u001b[39m tables = camelot.read_pdf(pdf_path,\n\u001b[32m     22\u001b[39m                       pages=\u001b[38;5;28mstr\u001b[39m(page_num),\n\u001b[32m     23\u001b[39m                       flavor=\u001b[33m\"\u001b[39m\u001b[33mlattice\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# 或 \"stream\"\u001b[39;00m\n\u001b[32m     24\u001b[39m                       strip_text=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtables\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.df)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 处理文本与表格\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/cl/lib/python3.12/site-packages/camelot/core.py:938\u001b[39m, in \u001b[36mTableList.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):  \u001b[38;5;66;03m# noqa D105\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m938\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tables\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "all_tables = []\n",
    "\n",
    "with pdfplumber.open(\"../guideline.pdf\") as pdf:\n",
    "    for page_num, page in enumerate(pdf.pages, 1):\n",
    "        width = page.width\n",
    "        mid = width / 2  # 假设左右等宽分栏\n",
    "\n",
    "        # 裁出左栏、右栏两个独立区域\n",
    "        left = page.within_bbox((0, 0, mid, page.height))\n",
    "        right = page.within_bbox((mid, 0, width, page.height))\n",
    "\n",
    "        for region in [left, right]:\n",
    "            # 提取文字\n",
    "            # text = region.extract_text()\n",
    "            # 提取表格\n",
    "            tables = region.extract_tables()\n",
    "            # all_tables.append(tables)\n",
    "            # print(tables)\n",
    "\n",
    "            # 处理文本与表格\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "import camelot\n",
    "\n",
    "def split_and_parse(pdf_path):\n",
    "    tables = []\n",
    "    reader = PdfReader(pdf_path)\n",
    "\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        width = float(page.mediabox.width)\n",
    "        height = float(page.mediabox.height)\n",
    "\n",
    "        # 左半页\n",
    "        left_writer = PdfWriter()\n",
    "        page_left = page.clone()\n",
    "        page_left.mediabox.upper_right = (width / 2, height)\n",
    "        left_writer.add_page(page_left)\n",
    "        left_path = f\"left_{i}.pdf\"\n",
    "        with open(left_path, \"wb\") as f:\n",
    "            left_writer.write(f)\n",
    "\n",
    "        # 右半页\n",
    "        right_writer = PdfWriter()\n",
    "        page_right = page.clone()\n",
    "        page_right.mediabox.lower_left = (width / 2, 0)\n",
    "        right_writer.add_page(page_right)\n",
    "        right_path = f\"right_{i}.pdf\"\n",
    "        with open(right_path, \"wb\") as f:\n",
    "            right_writer.write(f)\n",
    "\n",
    "        # Camelot解析\n",
    "        for path in [left_path, right_path]:\n",
    "            try:\n",
    "                parsed = camelot.read_pdf(path, flavor=\"lattice\", pages=\"1\")\n",
    "                tables.extend(parsed)\n",
    "            finally:\n",
    "                os.remove(path)\n",
    "\n",
    "    return tables\n",
    "\n",
    "# 用法\n",
    "pdf_file = \"your_file.pdf\"\n",
    "tables = split_and_parse(pdf_file)\n",
    "for idx, table in enumerate(tables):\n",
    "    print(f\"Table {idx}\")\n",
    "    print(table.df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
