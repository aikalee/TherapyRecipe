{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "6b3fbc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from together import Together\n",
    "import json\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "608b35e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = Together(api_key=os.getenv(\"SECRET_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "41907fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_graph_metadata(graph):\n",
    "\n",
    "    url = \"https://pmc.ncbi.nlm.nih.gov/articles/PMC11351064/#\"\n",
    "\n",
    "    figure = graph.find_parent(\"figure\")\n",
    "    figure_flag = False\n",
    "\n",
    "    section = graph.find_parent(id=re.compile(r'^section\\d+-\\d+$'))\n",
    "    section_id = section.get(\"id\")\n",
    "    section_url = url + section_id\n",
    "\n",
    "    section_heading = section.find(\"h2\").get_text()\n",
    "    section_subheading = section.find(\"h3\").get_text()\n",
    "    headings = section_heading + \" > \" + section_subheading\n",
    "\n",
    "    attribution = \"\"\n",
    "\n",
    "    \n",
    "    if figure:\n",
    "\n",
    "        figure_flag = True\n",
    "\n",
    "        image_url = graph.get(\"src\")\n",
    "\n",
    "        name = figure.select_one(\".obj_head\").get_text()\n",
    "        all_p = [p.get_text() for p in figure.find_all(\"p\") if not p.attrs]\n",
    "        caption = all_p[0]\n",
    "        label = name + \" \" + caption\n",
    "\n",
    "        attribution = \"(\" + figure.select_one('[aria-label=\"Attribution\"]').get_text() + \")\"\n",
    "        number = \"_\".join(re.findall(r\"(.{1})\\.\", name)).lower()\n",
    "        referee_id = f\"figure_{number}\"\n",
    "        \n",
    "          \n",
    "    else:\n",
    "\n",
    "        image_url = graph.get(\"src\")\n",
    "\n",
    "        table_section = graph.find_parent(\"section\")\n",
    "\n",
    "        name = table_section.select_one(\".obj_head\").get_text()\n",
    "        caption = table_section.select_one(\".caption p\").get_text()\n",
    "        label = name + \" \" + caption\n",
    "\n",
    "        number = \"_\".join(re.findall(r\"(.{1})\\.\", name)).lower()\n",
    "        referee_id = f\"table_{number}\"\n",
    "    \n",
    "\n",
    "    return attribution, caption, figure_flag, headings, image_url, label, name, referee_id, section_url\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "13e7332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_text(url, figure_flag):\n",
    "    \n",
    "    table_description_prompt = \"\"\"\n",
    "    You are a vision-language model. Your task is to extract the data from the image of a table and convert it into \n",
    "    structured natural language. Describe the content of the table in full sentences, preserving the structure and\n",
    "    relationships.\n",
    "\n",
    "    Guidelines:\n",
    "    - A row should form a sentence.\n",
    "    - For each cell, put column header and data point in pairs, and seperate the column header and data point with colons.\n",
    "    - For each row, arrange the cells in a linear order, and seperate each cell with commas to form a sentence.\n",
    "    - Ignore any symbols in the tables.\n",
    "\n",
    "    The format should be as follow:\n",
    "    \n",
    "    Row 0 - column header1: data point[0][0],  column header2: data point[0][1]. \n",
    "    Row 1 - column header1: data point[1][0],  column header2: data point[1][1]. \n",
    "    \n",
    "    Ensure the output is suitable for downstream language models to process as if it were read by a human. \n",
    "    Be precise and complete. \n",
    "    Do not add any additional characters beside the description.\n",
    "    \"\"\"\n",
    "\n",
    "    figure_description_prompt = \"\"\"\n",
    "    You are a vision-language model. Your task is to extract and understand the flow of a process from a flowchart image. \n",
    "    Represent this process in a clear, linear text format using -> to denote transitions between steps.\n",
    "\n",
    "    Guidelines:\n",
    "\n",
    "    - Each step should be written in text, exactly as it appears in the flowchart.\n",
    "    - Use -> to indicate directional flow from one step to the next.\n",
    "    - Preserve loops or retries by showing steps that return to earlier points.\n",
    "    - Ensure the structure is readable and maintains the original logic of the flowchart.\n",
    "\n",
    "    Ensure the output is suitable for downstream language models to process as if it were read by a human. \n",
    "    Be precise and complete.\n",
    "    Do not add any additional characters beside the description.\n",
    "    \"\"\"\n",
    "\n",
    "    get_description_prompt = figure_description_prompt if figure_flag else table_description_prompt\n",
    "\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-Vision-Free\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": get_description_prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": url,\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        stream=False,\n",
    "    )\n",
    "# for chunk in stream:\n",
    "#     print(chunk.choices[0].delta.content or \"\" if chunk.choices else \"\", end=\"\", flush=True)\n",
    "    return stream.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "6f12ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_chunk(text_block, section_url, referee_id, headings):\n",
    "\n",
    "    d = {\n",
    "    \"text\": text_block,\n",
    "    \"metadata\": {\n",
    "        \"section\": section_url,\n",
    "        \"type\": \"table image\",\n",
    "        \"referee_id\": referee_id,\n",
    "        \"headings\": headings,\n",
    "        }\n",
    "    }\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2a88ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/raw/source.html', encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "    soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "4243295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = []\n",
    "for graph in soup.select(\".graphic\"):\n",
    "    attribution, caption, figure_flag, headings, image_url, label, name, referee_id, section_url = get_graph_metadata(graph)\n",
    "    text_block = label + \" \" + get_response_text(image_url, figure_flag) + \" \" + f\"{attribution}\"\n",
    "    chunk = to_chunk(text_block, section_url, referee_id, headings)\n",
    "    doc.append(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "50e59d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed/graphs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(doc, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51afe507",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
