{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87665019",
   "metadata": {},
   "source": [
    "# Load query and Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0caf472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#read in file\n",
    "with open(\"../../data/raw/queries.txt\", \"r\") as file:\n",
    "    queries = file.readlines()\n",
    "\n",
    "with open(\"../../data/raw/answers.txt\", \"r\") as file:\n",
    "    answers = file.readlines()\n",
    "    \n",
    "with open(\"../../data/processed/guideline_db_with_table.json\") as f:\n",
    "    db_raw = json.load(f)\n",
    "    \n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    section: str\n",
    "    type: str\n",
    "    chunk_index: Optional[int]= None\n",
    "    headings: str\n",
    "    referee_id: Optional[str] = None\n",
    "    referenced_tables: Optional[List[str]] = None\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    text: str\n",
    "    metadata: Metadata\n",
    "\n",
    "\n",
    "db = [Chunk(**chunk) for chunk in db_raw]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91abea",
   "metadata": {},
   "source": [
    "# Make embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a768818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3887"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.mps.empty_cache()\n",
    "# del embedder\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2290bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#Make embeddings\n",
    "embedder_name = \"ba\"\n",
    "embedder = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "texts = [chunk.text for chunk in db]\n",
    "embeddings = embedder.encode(texts, convert_to_numpy=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d63a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if MPS OOM, run this instead\n",
    "embeddings = embedder.encode(\n",
    "    texts,\n",
    "    convert_to_numpy=True,\n",
    "    device='cpu',\n",
    "    batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c240d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(f\"{embedder_name.replace(\"/\", \"_\")}.npy\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec39e82",
   "metadata": {},
   "source": [
    "# Build faiss index and implement Search using faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2822bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # âœ… Caching mechanism\n",
    "# if os.path.exists(\"vectors.npy\") and os.path.exists(\"vectors.md5\"):\n",
    "#     with open(\"vectors.md5\", \"r\") as f:\n",
    "#         saved_md5 = f.read().strip()\n",
    "#     if saved_md5 == json_md5:\n",
    "#         print(\"âœ… Loaded cached vectors.\")\n",
    "#         vectors = np.load(\"vectors.npy\")\n",
    "#     else:\n",
    "#         print(\"ðŸ”„ Manual updated. Recomputing vectors...\")\n",
    "#         vectors = embed_chunks(chunks)\n",
    "#         np.save(\"vectors.npy\", vectors)\n",
    "#         with open(\"vectors.md5\", \"w\") as f:\n",
    "#             f.write(json_md5)\n",
    "# else:\n",
    "#     print(\"ðŸ”„ No cached vectors found. Computing now...\")\n",
    "#     vectors = embed_chunks(chunks)\n",
    "#     np.save(\"vectors.npy\", vectors)\n",
    "#     with open(\"vectors.md5\", \"w\") as f:\n",
    "#         f.write(json_md5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58744381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build faiss index\n",
    "import faiss\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance\n",
    "index.add(embeddings)  # Add embeddings to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64f87d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "llm_client = Together(api_key='4f6e44b7689d6592b2b5b57ad3940ac9f488d14c22802e8bcdf641b06e98cbbe')\n",
    "#4f6e44b7689d6592b2b5b57ad3940ac9f488d14c22802e8bcdf641b06e98cbbe\n",
    "\n",
    "def faiss_search(query, k=3):\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = []\n",
    "    referenced_tables = set()\n",
    "    existed_tables = set()\n",
    "    for i in range(k):\n",
    "        if indices[0][i] != -1:  # Check if the index is valid\n",
    "            results.append({\n",
    "                \"text\": db[indices[0][i]].text,\n",
    "                \"section\": db[indices[0][i]].metadata.section,\n",
    "            })\n",
    "        # if this chunk has a referee_id, it is a table already, we don't need to add it again later\n",
    "        if db[indices[0][i]].metadata.referee_id:\n",
    "            existed_tables.add(db[indices[0][i]].metadata.referee_id)\n",
    "        if db[indices[0][i]].metadata.referenced_tables:\n",
    "            referenced_tables.update(db[indices[0][i]].metadata.referenced_tables)\n",
    "        \n",
    "        #perform .lower().replace(\" \", \"_\").replace(\".\", \"_\") to all the table in the referenced_tables\n",
    "    table_to_add = {table.lower().replace(\" \", \"_\").replace(\".\", \"_\") for table in referenced_tables if table not in existed_tables}\n",
    "    \n",
    "    # add the referenced tables in the db to the results if their referee_id is in table_to_add\n",
    "    i = 0\n",
    "    for chunk in db:\n",
    "        if chunk.metadata.referee_id in table_to_add:\n",
    "            results.append({\n",
    "                \"text\": chunk.text,\n",
    "                \"section\": chunk.metadata.section,\n",
    "            })\n",
    "            i += 1\n",
    "        if i == len(table_to_add):\n",
    "            break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3449e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt):\n",
    "\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", #don't change the model!\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0.05\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1da8052a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (auto_model): XLMRobertaLoRA(\n",
       "      (roberta): XLMRobertaModel(\n",
       "        (embeddings): XLMRobertaEmbeddings(\n",
       "          (word_embeddings): ParametrizedEmbedding(\n",
       "            250002, 1024, padding_idx=1\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): LoRAParametrization()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (token_type_embeddings): ParametrizedEmbedding(\n",
       "            1, 1024\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): LoRAParametrization()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (emb_drop): Dropout(p=0.1, inplace=False)\n",
       "        (emb_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): XLMRobertaEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x Block(\n",
       "              (mixer): MHA(\n",
       "                (rotary_emb): RotaryEmbedding()\n",
       "                (Wqkv): ParametrizedLinearResidual(\n",
       "                  in_features=1024, out_features=3072, bias=True\n",
       "                  (parametrizations): ModuleDict(\n",
       "                    (weight): ParametrizationList(\n",
       "                      (0): LoRAParametrization()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (inner_attn): SelfAttention(\n",
       "                  (drop): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (inner_cross_attn): CrossAttention(\n",
       "                  (drop): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (out_proj): ParametrizedLinear(\n",
       "                  in_features=1024, out_features=1024, bias=True\n",
       "                  (parametrizations): ModuleDict(\n",
       "                    (weight): ParametrizationList(\n",
       "                      (0): LoRAParametrization()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): ParametrizedLinear(\n",
       "                  in_features=1024, out_features=4096, bias=True\n",
       "                  (parametrizations): ModuleDict(\n",
       "                    (weight): ParametrizationList(\n",
       "                      (0): LoRAParametrization()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (fc2): ParametrizedLinear(\n",
       "                  in_features=4096, out_features=1024, bias=True\n",
       "                  (parametrizations): ModuleDict(\n",
       "                    (weight): ParametrizationList(\n",
       "                      (0): LoRAParametrization()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): XLMRobertaPooler(\n",
       "          (dense): ParametrizedLinear(\n",
       "            in_features=1024, out_features=1024, bias=True\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): LoRAParametrization()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (normalizer): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "296bfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(query, faiss_results):\n",
    "    system_prompt = (\n",
    "        \"Your name is Depression Assistant, a helpful and friendly recipe assistant. \"\n",
    "        \"Summarize the clinical guidelines provided in the context and then tried to answer the user query. \"\n",
    "        \"If the query or guideline provided is not related to depression, please say 'I am not sure about that'. Don't make up things. \"\n",
    "        \n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "### System Prompt\n",
    "{system_prompt}\n",
    "\n",
    "### User Query\n",
    "{query}\n",
    "\n",
    "### Clinical Guidelines Context\n",
    "    \"\"\"\n",
    "    for result in faiss_results:\n",
    "        prompt += f\"- reference: {result['section']}\\n- This paragraph is from section{result['text']}\\n\"\n",
    "        \n",
    "    return prompt\n",
    "\n",
    "import time\n",
    "\n",
    "def depression_assistant(query):\n",
    "    t1 = time.perf_counter()\n",
    "    \n",
    "    results = faiss_search(query)\n",
    "    t2 = time.perf_counter()\n",
    "    print(f\"[Time] FAISS search done in {t2 - t1:.2f} seconds.\")\n",
    "\n",
    "    prompt = construct_prompt(query, results)\n",
    "    t3 = time.perf_counter()\n",
    "    print(f\"[Time] Prompt construction took {t3 - t2:.2f} seconds.\")\n",
    "\n",
    "    response = call_llm(prompt)\n",
    "    t4 = time.perf_counter()\n",
    "    print(f\"[Time] LLM response took {t4 - t3:.2f} seconds.\")\n",
    "\n",
    "    print(f\"[Total time] {t4 - t1:.2f} seconds for this query.\\n\\n\")\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e8ab4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time] FAISS search done in 0.30 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 2.11 seconds.\n",
      "[Total time] 2.41 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.26 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 2.19 seconds.\n",
      "[Total time] 2.45 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.21 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 15.41 seconds.\n",
      "[Total time] 15.62 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.18 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 3.10 seconds.\n",
      "[Total time] 3.28 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.19 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 15.57 seconds.\n",
      "[Total time] 15.75 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.31 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 2.18 seconds.\n",
      "[Total time] 2.50 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.28 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 10.76 seconds.\n",
      "[Total time] 11.04 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.17 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 3.49 seconds.\n",
      "[Total time] 3.66 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.22 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 6.87 seconds.\n",
      "[Total time] 7.09 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.48 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 8.29 seconds.\n",
      "[Total time] 8.77 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.31 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 9.47 seconds.\n",
      "[Total time] 9.78 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.26 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 12.66 seconds.\n",
      "[Total time] 12.92 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.26 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 3.37 seconds.\n",
      "[Total time] 3.64 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.25 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 1.90 seconds.\n",
      "[Total time] 2.15 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.21 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 20.19 seconds.\n",
      "[Total time] 20.39 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.25 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 5.53 seconds.\n",
      "[Total time] 5.78 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.17 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 7.36 seconds.\n",
      "[Total time] 7.53 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.19 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 12.08 seconds.\n",
      "[Total time] 12.27 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.20 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 11.62 seconds.\n",
      "[Total time] 11.82 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.23 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 5.58 seconds.\n",
      "[Total time] 5.81 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.14 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 4.37 seconds.\n",
      "[Total time] 4.50 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.59 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 3.51 seconds.\n",
      "[Total time] 4.10 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.14 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 20.51 seconds.\n",
      "[Total time] 20.65 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.15 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 7.26 seconds.\n",
      "[Total time] 7.41 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.14 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 7.64 seconds.\n",
      "[Total time] 7.78 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.13 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 11.74 seconds.\n",
      "[Total time] 11.88 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.15 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 3.33 seconds.\n",
      "[Total time] 3.48 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.19 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 10.74 seconds.\n",
      "[Total time] 10.93 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.16 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 4.68 seconds.\n",
      "[Total time] 4.84 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.14 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 5.80 seconds.\n",
      "[Total time] 5.94 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.14 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 4.37 seconds.\n",
      "[Total time] 4.50 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.14 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 10.93 seconds.\n",
      "[Total time] 11.06 seconds for this query.\n",
      "\n",
      "\n",
      "[Time] FAISS search done in 0.15 seconds.\n",
      "[Time] Prompt construction took 0.00 seconds.\n",
      "[Time] LLM response took 4.15 seconds.\n",
      "[Total time] 4.30 seconds for this query.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{embedder_name.replace(\"/\", \"_\")}_llama3.3_70B.md\", \"w\") as f:\n",
    "    for i, query in enumerate(queries):\n",
    "        response = depression_assistant(query)\n",
    "        # write the response to a md file\n",
    "        f.write(f\"## Query {i+1}\\n\")\n",
    "        f.write(f\"{query.strip()}\\n\\n\")\n",
    "        f.write(\"#### Answer\\n\")\n",
    "        f.write(f\"{answers[i].strip()}\\n\\n\")\n",
    "        f.write(f\"#### {embedder_name} Embedder and LLama3.3 70B Response\\n\")\n",
    "        f.write(response.strip())\n",
    "        f.write(\"\\n\\n---\\n\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33d5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
