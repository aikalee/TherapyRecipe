{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87665019",
   "metadata": {},
   "source": [
    "# Load query and Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0caf472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#read in file\n",
    "with open(\"../../data/raw/queries.txt\", \"r\") as file:\n",
    "    queries = file.readlines()\n",
    "\n",
    "with open(\"../../data/raw/answers.txt\", \"r\") as file:\n",
    "    answers = file.readlines()\n",
    "    \n",
    "with open(\"../../data/processed/guideline_db_with_table.json\") as f:\n",
    "    db_raw = json.load(f)\n",
    "    \n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    section: str\n",
    "    type: str\n",
    "    chunk_index: Optional[int]= None\n",
    "    headings: str\n",
    "    referee_id: Optional[str] = None\n",
    "    referenced_tables: Optional[List[str]] = None\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    text: str\n",
    "    metadata: Metadata\n",
    "\n",
    "\n",
    "db = [Chunk(**chunk) for chunk in db_raw]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91abea",
   "metadata": {},
   "source": [
    "# Make embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2290bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#Make embeddings\n",
    "embedder_name = 'abhinand/MedEmbed-large-v0.1'\n",
    "embedder = SentenceTransformer(embedder_name)\n",
    "\n",
    "texts = [chunk.text for chunk in db]\n",
    "embeddings = embedder.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "import numpy as np\n",
    "np.save(embedder_name+\".npy\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec39e82",
   "metadata": {},
   "source": [
    "# Build faiss index and implement Search using faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2822bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # âœ… Caching mechanism\n",
    "# if os.path.exists(\"vectors.npy\") and os.path.exists(\"vectors.md5\"):\n",
    "#     with open(\"vectors.md5\", \"r\") as f:\n",
    "#         saved_md5 = f.read().strip()\n",
    "#     if saved_md5 == json_md5:\n",
    "#         print(\"âœ… Loaded cached vectors.\")\n",
    "#         vectors = np.load(\"vectors.npy\")\n",
    "#     else:\n",
    "#         print(\"ðŸ”„ Manual updated. Recomputing vectors...\")\n",
    "#         vectors = embed_chunks(chunks)\n",
    "#         np.save(\"vectors.npy\", vectors)\n",
    "#         with open(\"vectors.md5\", \"w\") as f:\n",
    "#             f.write(json_md5)\n",
    "# else:\n",
    "#     print(\"ðŸ”„ No cached vectors found. Computing now...\")\n",
    "#     vectors = embed_chunks(chunks)\n",
    "#     np.save(\"vectors.npy\", vectors)\n",
    "#     with open(\"vectors.md5\", \"w\") as f:\n",
    "#         f.write(json_md5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58744381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build faiss index\n",
    "import faiss\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance\n",
    "index.add(embeddings)  # Add embeddings to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f87d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "llm_client = Together(api_key='4f6e44b7689d6592b2b5b57ad3940ac9f488d14c22802e8bcdf641b06e98cbbe')\n",
    "#4f6e44b7689d6592b2b5b57ad3940ac9f488d14c22802e8bcdf641b06e98cbbe\n",
    "\n",
    "def faiss_search(query, k=3):\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = []\n",
    "    referenced_tables = set()\n",
    "    existed_tables = set()\n",
    "    for i in range(k):\n",
    "        if indices[0][i] != -1:  # Check if the index is valid\n",
    "            results.append({\n",
    "                \"text\": db[indices[0][i]].text,\n",
    "                \"section\": db[indices[0][i]].metadata.section,\n",
    "            })\n",
    "        # if this chunk has a referee_id, it is a table already, we don't need to add it again later\n",
    "        if db[indices[0][i]].metadata.referee_id:\n",
    "            existed_tables.add(db[indices[0][i]].metadata.referee_id)\n",
    "        if db[indices[0][i]].metadata.referenced_tables:\n",
    "            referenced_tables.update(db[indices[0][i]].metadata.referenced_tables)\n",
    "        \n",
    "        #perform .lower().replace(\" \", \"_\").replace(\".\", \"_\") to all the table in the referenced_tables\n",
    "    table_to_add = {table.lower().replace(\" \", \"_\").replace(\".\", \"_\") for table in referenced_tables if table not in existed_tables}\n",
    "    \n",
    "    # add the referenced tables in the db to the results if their referee_id is in table_to_add\n",
    "    i = 0\n",
    "    for chunk in db:\n",
    "        if chunk.metadata.referee_id in table_to_add:\n",
    "            results.append({\n",
    "                \"text\": chunk.text,\n",
    "                \"section\": chunk.metadata.section,\n",
    "            })\n",
    "            i += 1\n",
    "        if i == len(table_to_add):\n",
    "            break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3449e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt):\n",
    "\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", #don't change the model!\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0.05\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296bfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(query, faiss_results):\n",
    "    system_prompt = (\n",
    "        \"Your name is Depression Assistant, a helpful and friendly recipe assistant. \"\n",
    "        \"Summarize the clinical guidelines provided in the context and then tried to answer the user query. \"\n",
    "        \"If the query or guideline provided is not related to depression, please say 'I am not sure about that'. Don't make up things. \"\n",
    "        \n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "### System Prompt\n",
    "{system_prompt}\n",
    "\n",
    "### User Query\n",
    "{query}\n",
    "\n",
    "### Clinical Guidelines Context\n",
    "    \"\"\"\n",
    "    for result in faiss_results:\n",
    "        prompt += f\"- reference: {result['section']}\\n- This paragraph is from section{result['text']}\\n\"\n",
    "        \n",
    "    return prompt\n",
    "\n",
    "import time\n",
    "embedder = SentenceTransformer(embedder_name)\n",
    "\n",
    "def depression_assistant(query):\n",
    "    t0 = time.perf_counter()\n",
    "    \n",
    "    results = faiss_search(query)\n",
    "    t2 = time.perf_counter()\n",
    "    print(f\"[Time] FAISS search done in {t2 - t1:.2f} seconds.\")\n",
    "\n",
    "    prompt = construct_prompt(query, results)\n",
    "    t3 = time.perf_counter()\n",
    "    print(f\"[Time] Prompt construction took {t3 - t2:.2f} seconds.\")\n",
    "\n",
    "    response = call_llm(prompt)\n",
    "    t4 = time.perf_counter()\n",
    "    print(f\"[Time] LLM response took {t4 - t3:.2f} seconds.\")\n",
    "\n",
    "    print(f\"[Total time] {t4 - t0:.2f} seconds for this query.\\n\\n\")\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ab4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{embedder_name}_llama3.3_70B.md\", \"w\") as f:\n",
    "    for i, query in enumerate(queries):\n",
    "        response = depression_assistant(query)\n",
    "        # write the response to a md file\n",
    "        f.write(f\"## Query {i+1}\\n\")\n",
    "        f.write(f\"{query.strip()}\\n\\n\")\n",
    "        f.write(\"#### Answer\\n\")\n",
    "        f.write(f\"{answers[i].strip()}\\n\\n\")\n",
    "        f.write(\"#### Response\\n\")\n",
    "        f.write(response.strip())\n",
    "        f.write(\"\\n\\n---\\n\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33d5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
